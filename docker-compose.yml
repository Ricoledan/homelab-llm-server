services:
  llama-server:
    build: .
    env_file:
      - .env
    container_name: "${CONTAINER_NAME:-llama_server}"
    restart: unless-stopped
    ports:
      - "${SERVER_PORT:-8080}:${SERVER_PORT:-8080}"
    volumes:
      - "${MODEL_DIR}:/app/models:rw"
    environment:
      - MODEL_PATH=/app/models/${MODEL_FILENAME}
      - SERVER_PORT=${SERVER_PORT:-8080}
      - ROC_ENABLE=${ROC_ENABLE:-1}
      - ROCM_VISIBLE_DEVICES=all
      - CONTEXT_SIZE=${CONTEXT_SIZE:-4096}
      - N_GPU_LAYERS=${N_GPU_LAYERS:-50}
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    security_opt:
      - seccomp=unconfined
    group_add:
      - video